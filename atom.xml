<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://dayuebai.github.io</id>
    <title>Dayue&apos;s Learning Blog</title>
    <updated>2021-02-02T04:07:58.726Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://dayuebai.github.io"/>
    <link rel="self" href="https://dayuebai.github.io/atom.xml"/>
    <subtitle>Stay hungry. Stay foolish.</subtitle>
    <logo>https://dayuebai.github.io/images/avatar.png</logo>
    <icon>https://dayuebai.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, Dayue&apos;s Learning Blog</rights>
    <entry>
        <title type="html"><![CDATA[Hadoop Cluster Setup Tutorial (For Beginners)]]></title>
        <id>https://dayuebai.github.io/post/hadoop-cluster-setup-tutorial-for-beginners/</id>
        <link href="https://dayuebai.github.io/post/hadoop-cluster-setup-tutorial-for-beginners/">
        </link>
        <updated>2020-11-24T13:45:46.000Z</updated>
        <content type="html"><![CDATA[<h2 id="supported-platform">Supported Platform</h2>
<ul>
<li>
<p>GNU/Linux is supported as a development and production platform. Hadoop has been demonstrated on GNU/Linux clusters with 2000 nodes.</p>
</li>
<li>
<p>Windows is also a supported platform, but this guide only demonstrates how to install Hadoop on Linux.</p>
</li>
</ul>
<h2 id="required-software">Required Software</h2>
<p>Required software for Linux include:</p>
<ul>
<li>
<p><strong>Java</strong> must be installed (Java 7 is recommended for newer versions).</p>
</li>
<li>
<p><strong>ssh</strong> must be installed and sshd must be running to use the Hadoop scripts that manage remote Hadoop daemons if the optional start and stop scripts are to be used.</p>
</li>
<li>
<p><strong>pdsh</strong> is also recommended to be installed for better ssh resource management.</p>
<ul>
<li>run: <code>sudo yum install pdsh</code> on UIUC VM (OS: RHEL).</li>
</ul>
</li>
</ul>
<h2 id="download-hadoop-version-277">Download Hadoop (Version 2.7.7)</h2>
<ul>
<li>
<p>Download Hadoop source package: <code>wget https://archive.apache.org/dist/hadoop/core/hadoop-2.7.7/hadoop-2.7.7.tar.gz</code></p>
</li>
<li>
<p>Unpack the installed package: <code>tar -xvf hadoop-2.7.7.tar.gz</code></p>
</li>
<li>
<p>Change directory: <code>cd hadoop-2.7.7/</code></p>
</li>
</ul>
<h2 id="hadoop-cluster-configuration">Hadoop Cluster Configuration</h2>
<p>Step 1. Update <code>~/.bashrc</code> by adding the following lines:</p>
<pre><code>export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.272.b10-1.el7_9.x86_64/jre
export PATH=$PATH:$JAVA_HOME/bin
export HADOOP_HOME=$HOME/hadoop-2.7.7
export HADOOP_CONF_DIR=$HOME/hadoop-2.7.7/etc/hadoop
export HADOOP_MAPRED_HOME=$HOME/hadoop-2.7.7
export HADOOP_COMMON_HOME=$HOME/hadoop-2.7.7
export HADOOP_HDFS_HOME=$HOME/hadoop-2.7.7
export YARN_HOME=$HOME/hadoop-2.7.7
export PATH=$PATH:$HOME/hadoop-2.7.7/bin
export HADOOP_CLASSPATH=/usr/lib/jvm/java-openjdk/lib/tools.jar
</code></pre>
<hr>
<p>Also update <code>JAVA_HOME</code> path in the file: <code>$HOME/hadoop-2.7.7/etc/hadoop/hadoop-env.sh</code> as follows.</p>
<pre><code>export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.272.b10-1.el7_9.x86_64/jre
export PATH=$PATH:$JAVA_HOME/bin
</code></pre>
<p>Step 2. Run <code>source ~/.bashrc</code> command</p>
<p>Step 3. Copy the master node's ssh key to slave's authorized keys.</p>
<p><code>ssh-copy-id -i $HOME/.ssh/id_rsa.pub username@slave</code></p>
<p>Step 4. On the <strong>master</strong> node, create a file named <code>masters</code> under path: <code>$HOME/hadoop-2.7.7/etc/hadoop</code></p>
<p>Step 5. Add master node's public IP address to the file just created.</p>
<p>Step 6. Add the IPs of all slaves to the file at path: <code>$HOME/hadoop-2.7.7/etc/hadoop/slaves</code> on the <strong>master</strong> node only.</p>
<p>Step 7. Update <code>$HOME/hadoop-2.7.7/etc/hadoop/core-site.xml</code> on <strong>all</strong> nodes in the cluster with the following lines:</p>
<pre><code class="language-xml">&lt;configuration&gt;
	&lt;property&gt;
		&lt;name&gt;fs.default.name&lt;/name&gt;
		&lt;value&gt;hdfs://&lt;MASTER_IP_ADDR&gt;:9000&lt;/value&gt;
	&lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p>Step 8. Update the file at path: <code>$HOME/hadoop-2.7.7/etc/hadoop/hdfs-site.xml</code> on the master node as follows:</p>
<pre><code class="language-xml">&lt;configuration&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.replication&lt;/name&gt;
		&lt;value&gt;3&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.permissions&lt;/name&gt;
		&lt;value&gt;false&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
		&lt;value&gt;&lt;hadoop-path&gt;/namenode&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
		&lt;value&gt;&lt;hadoop-path&gt;/hadoop-2.7.7/datanode&lt;/value&gt;
	&lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p>Step 9. Update the file at path: <code>$HOME/hadoop-2.7.7/etc/hadoop/hdfs-site.xml</code> on the slave node as follows:</p>
<pre><code class="language-xml">&lt;configuration&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.replication&lt;/name&gt;
		&lt;value&gt;3&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.permissions&lt;/name&gt;
		&lt;value&gt;false&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
		&lt;value&gt;&lt;hadoop_path&gt;/datanode&lt;/value&gt;
	&lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p>Step 10. For all nodes in the cluster, create a new file named <code>mapred-site.xml</code> under path: <code>$HOME/hadoop-2.7.7/etc/hadoop/</code> and add the following lines to the file.</p>
<pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
&lt;!--
  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
--&gt;
&lt;!-- Put site-specific property overrides in this file. --&gt;
&lt;configuration&gt;
	&lt;property&gt;
		&lt;name&gt;mapreduce.framework.name&lt;/name&gt;
		&lt;value&gt;yarn&lt;/value&gt;
	&lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p>Step 11. For all nodes in the cluster, update the file: <code>$HOME/hadoop-2.7.7/etc/hadoop/yarn-site.xml</code> with the following lines.</p>
<pre><code class="language-xml">&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
    &lt;value&gt;172.22.94.103:8032&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
    &lt;value&gt;172.22.94.103:8030&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
    &lt;value&gt;172.22.94.103:8031&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;
    &lt;value&gt;172.22.94.103:8033&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;
    &lt;value&gt;172.22.94.103:8088&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.auxservices.mapreduce.shuffle.class&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<p>Step 12. On the <strong>master</strong> master (only), format the namenode using the command: <code>hadoop namenode -format</code></p>
<p>Step 13. To start hadoop cluster, run <code>./sbin/start-dfs.sh</code> and <code>./sbin/start-yarn.sh</code> on the <strong>master</strong> node only.</p>
<h2 id="quick-start-example">Quick Start Example</h2>
<p>Step 1. Create a file named: <code>WordCount.java</code> and copy the following code block to this new file. WordCount is a simple application that counts the number of occurrences of each word in a given input set.</p>
<pre><code class="language-java">import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {

  public static class TokenizerMapper
       extends Mapper&lt;Object, Text, Text, IntWritable&gt;{

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
      }
    }
  }

  public static class IntSumReducer
       extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable&lt;IntWritable&gt; values,
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }

  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, &quot;word count&quot;);
    job.setJarByClass(WordCount.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}
</code></pre>
<p>Step 2. Compile WordCount.java and create a jar:</p>
<pre><code>bin/hadoop com.sun.tools.javac.Main WordCount.java

jar cf wc.jar WordCount*.class
</code></pre>
<p>Step 3. Add files: <code>file01</code> and <code>file02</code> under <strong>local</strong> directory: <code>wc-input</code>. Sample text-files as input:</p>
<pre><code>hadoop fs -mkdir -p /wc/input
hadoop fs -put wc-input/* /wc/input

hadoop fs -ls /wc/input
</code></pre>
<p>Step 4. Run applications using MapReduce of Hadoop:</p>
<pre><code>hadoop jar wc.jar WordCount /wc/input /wc/output
hadoop fs -ls /wc/output
hadoop fs -get /wc/output/part-r-00000 wc-output/output.txt
</code></pre>
<p>Results will be downloaded to the local file: <code>output.txt</code></p>
<h2 id="faq">FAQ</h2>
<ol>
<li>What to do if you run <code>jps</code> on slave nodes and cannot find <code>DataNode</code> process running?</li>
</ol>
<ul>
<li>Delete <code>datanode/</code> directory on slave nodes first</li>
<li>Re-run format command: <code>hadoop namenode -format</code> on the master node</li>
<li>Start the cluster again then you will see the <code>DataNode</code> process.</li>
</ul>
<ol start="2">
<li>How to debug the cluster setting issue?</li>
</ol>
<ul>
<li><strong>Always</strong> check log files.</li>
</ul>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://hadoop.apache.org/docs/stable/index.html">Hadoop Getting Started</a></li>
<li><a href="https://hadoop.apache.org/docs/r2.9.2/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html">Hadoop MapReduce Tutorial</a></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Probability Question (Solved on 11/01/2020)]]></title>
        <id>https://dayuebai.github.io/post/probability-question/</id>
        <link href="https://dayuebai.github.io/post/probability-question/">
        </link>
        <updated>2020-11-01T10:20:13.000Z</updated>
        <content type="html"><![CDATA[<p>There are 15 apples in total. 4 apples go bad and the others are all fresh apples. Calculate the probability that: we picked the last bad apple at exactly the 9th pick.</p>
<p>Solution:</p>
<p>Assume that all of the apples are different.<br>
Event Q: we picked the last bad apple at exactly the 9th pick.</p>
<p>P(Q) = #(3 bad apples are picked within first 8 picks and the 9th pick is also a bad apple) / sample space</p>
<p>sample space = #(possible order of 15 apples) = 15!</p>
<p>P(Q) = C(8, 3) * P(4, 4) * P(11, 11) / 15! = 8 / 195</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello World Blog]]></title>
        <id>https://dayuebai.github.io/post/about/</id>
        <link href="https://dayuebai.github.io/post/about/">
        </link>
        <updated>2019-01-21T23:09:48.000Z</updated>
        <content type="html"><![CDATA[<p>TODO</p>
]]></content>
    </entry>
</feed>