<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://dayuebai.github.io</id>
    <title>Dayue&apos;s Blog</title>
    <updated>2020-12-02T04:33:48.139Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://dayuebai.github.io"/>
    <link rel="self" href="https://dayuebai.github.io/atom.xml"/>
    <subtitle>Stay hungry. Stay foolish.</subtitle>
    <logo>https://dayuebai.github.io/images/avatar.png</logo>
    <icon>https://dayuebai.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, Dayue&apos;s Blog</rights>
    <entry>
        <title type="html"><![CDATA[Hadoop Cluster Setup Tutorial (For Beginners)]]></title>
        <id>https://dayuebai.github.io/post/hadoop-cluster-setup-tutorial-for-beginners/</id>
        <link href="https://dayuebai.github.io/post/hadoop-cluster-setup-tutorial-for-beginners/">
        </link>
        <updated>2020-11-25T03:45:46.000Z</updated>
        <content type="html"><![CDATA[<h2 id="supported-platform">Supported Platform</h2>
<ul>
<li>
<p>GNU/Linux is supported as a development and production platform. Hadoop has been demonstrated on GNU/Linux clusters with 2000 nodes.</p>
</li>
<li>
<p>Windows is also a supported platform, but this guide only demonstrates how to install Hadoop on Linux.</p>
</li>
</ul>
<h2 id="required-software">Required Software</h2>
<p>Required software for Linux include:</p>
<ul>
<li>
<p><strong>Java</strong> must be installed (Java 7 is recommended for newer versions).</p>
</li>
<li>
<p><strong>ssh</strong> must be installed and sshd must be running to use the Hadoop scripts that manage remote Hadoop daemons if the optional start and stop scripts are to be used.</p>
</li>
<li>
<p><strong>pdsh</strong> is also recommended to be installed for better ssh resource management.</p>
<ul>
<li>run: <code>sudo yum install pdsh</code> on UIUC VM (OS: RHEL).</li>
</ul>
</li>
</ul>
<h2 id="download-hadoop-version-277">Download Hadoop (Version 2.7.7)</h2>
<ul>
<li>
<p>Download Hadoop source package: <code>wget https://archive.apache.org/dist/hadoop/core/hadoop-2.7.7/hadoop-2.7.7.tar.gz</code></p>
</li>
<li>
<p>Unpack the installed package: <code>tar -xvf hadoop-2.7.7.tar.gz</code></p>
</li>
<li>
<p>Change directory: <code>cd hadoop-2.7.7/</code></p>
</li>
</ul>
<h2 id="hadoop-cluster-configuration">Hadoop Cluster Configuration</h2>
<p>Step 1. Update <code>~/.bashrc</code> by adding the following lines:</p>
<pre><code>export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.272.b10-1.el7_9.x86_64/jre
export PATH=$PATH:$JAVA_HOME/bin
export HADOOP_HOME=$HOME/hadoop-2.7.7
export HADOOP_CONF_DIR=$HOME/hadoop-2.7.7/etc/hadoop
export HADOOP_MAPRED_HOME=$HOME/hadoop-2.7.7
export HADOOP_COMMON_HOME=$HOME/hadoop-2.7.7
export HADOOP_HDFS_HOME=$HOME/hadoop-2.7.7
export YARN_HOME=$HOME/hadoop-2.7.7
export PATH=$PATH:$HOME/hadoop-2.7.7/bin
export HADOOP_CLASSPATH=/usr/lib/jvm/java-openjdk/lib/tools.jar
</code></pre>
<hr>
<p>Also update <code>JAVA_HOME</code> path in the file: <code>$HOME/hadoop-2.7.7/etc/hadoop/hadoop-env.sh</code> as follows.</p>
<pre><code>export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.272.b10-1.el7_9.x86_64/jre
export PATH=$PATH:$JAVA_HOME/bin
</code></pre>
<p>Step 2. Run <code>source ~/.bashrc</code> command</p>
<p>Step 3. Copy the master node's ssh key to slave's authorized keys.</p>
<p><code>ssh-copy-id -i $HOME/.ssh/id_rsa.pub username@slave</code></p>
<p>Step 4. On the <strong>master</strong> node, create a file named <code>masters</code> under path: <code>$HOME/hadoop-2.7.7/etc/hadoop</code></p>
<p>Step 5. Add master node's public IP address to the file just created.</p>
<p>Step 6. Add the IPs of all slaves to the file at path: <code>$HOME/hadoop-2.7.7/etc/hadoop/slaves</code> on the <strong>master</strong> node only.</p>
<p>Step 7. Update <code>$HOME/hadoop-2.7.7/etc/hadoop/core-site.xml</code> on <strong>all</strong> nodes in the cluster with the following lines:</p>
<pre><code class="language-xml">&lt;configuration&gt;
	&lt;property&gt;
		&lt;name&gt;fs.default.name&lt;/name&gt;
		&lt;value&gt;hdfs://&lt;MASTER_IP_ADDR&gt;:9000&lt;/value&gt;
	&lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p>Step 8. Update the file at path: <code>$HOME/hadoop-2.7.7/etc/hadoop/hdfs-site.xml</code> on the master node as follows:</p>
<pre><code class="language-xml">&lt;configuration&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.replication&lt;/name&gt;
		&lt;value&gt;3&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.permissions&lt;/name&gt;
		&lt;value&gt;false&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
		&lt;value&gt;&lt;hadoop-path&gt;/namenode&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
		&lt;value&gt;&lt;hadoop-path&gt;/hadoop-2.7.7/datanode&lt;/value&gt;
	&lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p>Step 9. Update the file at path: <code>$HOME/hadoop-2.7.7/etc/hadoop/hdfs-site.xml</code> on the slave node as follows:</p>
<pre><code class="language-xml">&lt;configuration&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.replication&lt;/name&gt;
		&lt;value&gt;3&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.permissions&lt;/name&gt;
		&lt;value&gt;false&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
		&lt;value&gt;&lt;hadoop_path&gt;/datanode&lt;/value&gt;
	&lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p>Step 10. For all nodes in the cluster, create a new file named <code>mapred-site.xml</code> under path: <code>$HOME/hadoop-2.7.7/etc/hadoop/</code> and add the following lines to the file.</p>
<pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;
&lt;!--
  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
--&gt;
&lt;!-- Put site-specific property overrides in this file. --&gt;
&lt;configuration&gt;
	&lt;property&gt;
		&lt;name&gt;mapreduce.framework.name&lt;/name&gt;
		&lt;value&gt;yarn&lt;/value&gt;
	&lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p>Step 11. For all nodes in the cluster, update the file: <code>$HOME/hadoop-2.7.7/etc/hadoop/yarn-site.xml</code> with the following lines.</p>
<pre><code class="language-xml">&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
    &lt;value&gt;172.22.94.103:8032&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
    &lt;value&gt;172.22.94.103:8030&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
    &lt;value&gt;172.22.94.103:8031&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;
    &lt;value&gt;172.22.94.103:8033&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;
    &lt;value&gt;172.22.94.103:8088&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.auxservices.mapreduce.shuffle.class&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<p>Step 12. On the <strong>master</strong> master (only), format the namenode using the command: <code>hadoop namenode -format</code></p>
<p>Step 13. To start hadoop cluster, run <code>./sbin/start-dfs.sh</code> and <code>./sbin/start-yarn.sh</code> on the <strong>master</strong> node only.</p>
<h2 id="quick-start-example">Quick Start Example</h2>
<p>Step 1. Create a file named: <code>WordCount.java</code> and copy the following code block to this new file. WordCount is a simple application that counts the number of occurrences of each word in a given input set.</p>
<pre><code class="language-java">import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {

  public static class TokenizerMapper
       extends Mapper&lt;Object, Text, Text, IntWritable&gt;{

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
      }
    }
  }

  public static class IntSumReducer
       extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable&lt;IntWritable&gt; values,
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }

  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, &quot;word count&quot;);
    job.setJarByClass(WordCount.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}
</code></pre>
<p>Step 2. Compile WordCount.java and create a jar:</p>
<pre><code>bin/hadoop com.sun.tools.javac.Main WordCount.java

jar cf wc.jar WordCount*.class
</code></pre>
<p>Step 3. Add files: <code>file01</code> and <code>file02</code> under <strong>local</strong> directory: <code>wc-input</code>. Sample text-files as input:</p>
<pre><code>hadoop fs -mkdir -p /wc/input
hadoop fs -put wc-input/* /wc/input

hadoop fs -ls /wc/input
</code></pre>
<p>Step 4. Run applications using MapReduce of Hadoop:</p>
<pre><code>hadoop jar wc.jar WordCount /wc/input /wc/output
hadoop fs -ls /wc/output
hadoop fs -get /wc/output/part-r-00000 wc-output/output.txt
</code></pre>
<p>Results will be downloaded to the local file: <code>output.txt</code></p>
<h2 id="faq">FAQ</h2>
<ol>
<li>What to do if you run <code>jps</code> on slave nodes and cannot find <code>DataNode</code> process running?</li>
</ol>
<ul>
<li>Delete <code>datanode/</code> directory on slave nodes first</li>
<li>Re-run format command: <code>hadoop namenode -format</code> on the master node</li>
<li>Start the cluster again then you will see the <code>DataNode</code> process.</li>
</ul>
<ol start="2">
<li>How to debug the cluster setting issue?</li>
</ol>
<ul>
<li><strong>Always</strong> check log files.</li>
</ul>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://hadoop.apache.org/docs/stable/index.html">Hadoop Getting Started</a></li>
<li><a href="https://hadoop.apache.org/docs/r2.9.2/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html">Hadoop MapReduce Tutorial</a></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leetcode 5. Longest Palindromic String [Medium]]]></title>
        <id>https://dayuebai.github.io/post/leetcode-5-longest-palindromic-string-medium/</id>
        <link href="https://dayuebai.github.io/post/leetcode-5-longest-palindromic-string-medium/">
        </link>
        <updated>2020-11-02T02:56:59.000Z</updated>
        <content type="html"><![CDATA[<pre><code>Given a string s, return the longest palindromic substring in s.

 

Example 1:

Input: s = &quot;babad&quot;
Output: &quot;bab&quot;
Note: &quot;aba&quot; is also a valid answer.
Example 2:

Input: s = &quot;cbbd&quot;
Output: &quot;bb&quot;
Example 3:

Input: s = &quot;a&quot;
Output: &quot;a&quot;
Example 4:

Input: s = &quot;ac&quot;
Output: &quot;a&quot;
 

Constraints:

1 &lt;= s.length &lt;= 1000
s consist of only digits and English letters (lower-case and/or upper-case),
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Probability Question (Solved on 11/01/2020)]]></title>
        <id>https://dayuebai.github.io/post/probability-question/</id>
        <link href="https://dayuebai.github.io/post/probability-question/">
        </link>
        <updated>2020-11-01T10:20:13.000Z</updated>
        <content type="html"><![CDATA[<p>There are 15 apples in total. 4 apples go bad and the others are all fresh apples. Calculate the probability that: we picked the last bad apple at exactly the 9th pick.</p>
<p>Solution:</p>
<p>Assume that all of the apples are different.<br>
Event Q: we picked the last bad apple at exactly the 9th pick.</p>
<p>P(Q) = #(3 bad apples are picked within first 8 picks and the 9th pick is also a bad apple) / sample space</p>
<p>sample space = #(possible order of 15 apples) = 15!</p>
<p>P(Q) = C(8, 3) * P(4, 4) * P(11, 11) / 15! = 8 / 195</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Notes on Mutual Exclusion]]></title>
        <id>https://dayuebai.github.io/post/notes-on-mutual-exclusion/</id>
        <link href="https://dayuebai.github.io/post/notes-on-mutual-exclusion/">
        </link>
        <updated>2020-11-01T05:59:04.000Z</updated>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leetcode 3. Longest Substring Without Repeatable Characters [MEDIUM]]]></title>
        <id>https://dayuebai.github.io/post/leetcode-3-longest-substring-without-repeatable-characters-medium/</id>
        <link href="https://dayuebai.github.io/post/leetcode-3-longest-substring-without-repeatable-characters-medium/">
        </link>
        <updated>2020-10-31T13:07:12.000Z</updated>
        <content type="html"><![CDATA[<pre><code>Given a string s, find the length of the longest substring without repeating characters.


Example 1:

Input: s = &quot;abcabcbb&quot;
Output: 3
Explanation: The answer is &quot;abc&quot;, with the length of 3.
Example 2:

Input: s = &quot;bbbbb&quot;
Output: 1
Explanation: The answer is &quot;b&quot;, with the length of 1.
Example 3:

Input: s = &quot;pwwkew&quot;
Output: 3
Explanation: The answer is &quot;wke&quot;, with the length of 3.
Notice that the answer must be a substring, &quot;pwke&quot; is a subsequence and not a substring.
Example 4:

Input: s = &quot;&quot;
Output: 0
 

Constraints:

0 &lt;= s.length &lt;= 5 * 104
s consists of English letters, digits, symbols and spaces.
</code></pre>
<h2 id="solution-1-brute-force">Solution 1 (Brute force)</h2>
<p>To find longest substring without repeatable characters, the most naive solution is to<br>
1. first enumerate all possbile substrings<br>
2. check each substring if there exists repeatable characters in the substring.</p>
<p>Step 1 takes O(N^2) time to enumerate all substrings.<br>
Step 2 takes O(N) time check if a substring contains repeatable chracters with hashtable.</p>
<p>Overall, this naive solution takes O(N^3) time.</p>
<h2 id="solution-2-optimized-brute-force">Solution 2 (Optimized brute force)</h2>
<p>We can optimize our solution based on solution 1. For each starting index i, find the maximum value of starting j such that there are no repeatable characters within the range [i, j); when we find the first repeatable characters, stop executing and increment i. In this way, for eaching starting index i, we don't have to iterate j to the last character of the string. Instead, we can stop iterating j when we first find repeatable chracters (further iteration is redundant).</p>
<p>Java code:</p>
<pre><code>Optimized brute force solution
// Time: O(N^2)
// Space: O(128)
// class Solution {
//     public int lengthOfLongestSubstring(String s) {
//         int ans = 0;
        
//         for (int i = 0; i &lt; s.length(); i++) {
//             char[] seen = new char[128];
//             int j = i;
//             while (j &lt; s.length() &amp;&amp; seen[s.charAt(j)] == 0) {
//                 seen[s.charAt(j)] = 1;
//                 j++;
//             }
//             ans = Math.max(ans, j - i);
//         }
        
//         return ans;
//     }
// }
</code></pre>
<p>Analysis:<br>
1. Time<br>
There are N iterations in total (for loop). For each iteration, it loops at most 128 times because there are only 128 unique chracters in ascii table. Therefore the time complexity is O(N^2) =&gt; further reduced to O(N * 128)<br>
2. Space<br>
For each iteration, we keep track of characters that already appear in this iteration. We use an array to check (instead of hashtable, reason: array is cheap, hashtable is expensive). Overall, the space complexity is just the size of the tracking array =&gt; O(128)</p>
<h2 id="solution-3-hash-table-sliding-window">Solution 3 (Hash Table + Sliding Window)</h2>
<p>To further optimize this algorithm, we should think about how we can get the final answer using only one scan of the string. Suppose we have a sliding window [i, j] such that there are no repeatable chracters in the window. Then, the final answer is the maximum possible size of this sliding window.</p>
<p>How to maintain such a window?</p>
<ol>
<li>We keep iterating the ending point (j) of this sliding window. j monotonically increases.</li>
<li>Find the smallest possible value of i.</li>
</ol>
<p>Now, the key challenge is how to find the feasible i for each j?</p>
<ol>
<li>We use a hash table to store the last index of each character.</li>
<li>For an endpoint j (stores character c), find the last index of c (i_c). i should be i_c + 1</li>
</ol>
<p>However, there might be a case shown as below:<br>
<code>abba</code></p>
<p>for j = 3 (zero based), it find the last index of <code>a</code> is 0, so it set i = 0 + 1 = 1. However, the window[1, 3] still contains repeatable chracters.</p>
<p>How to solve this issue?<br>
Let i = Math.max(i, last occurrence index of c + 1) // TODO: need further explanation</p>
<p>Java code:</p>
<pre><code>class Solution {
    public int lengthOfLongestSubstring(String s) {
        int[] lastOccur = new int[128];
        Arrays.fill(lastOccur, -1);
        int ans = 0;
        
        for (int i = 0, j = 0; j &lt; s.length(); j++) {
            i = Math.max(i, lastOccur[s.charAt(j)] + 1);
            ans = Math.max(ans, j - i + 1);
            lastOccur[s.charAt(j)] = j;
        }
        
        return ans;
    }
}
</code></pre>
<p>Analysis:<br>
1. Time: O(N), reason: one time scan of the input array<br>
2. Space: O(128), reason: use array of size 128 as a hash table.</p>
<p>This article is just a review of solutions posted by <a href="https://zxi.mytechroad.com/blog/string/leetcode-3-longest-substring-without-repeating-characters/">huahua</a> . Thanks!</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[About]]></title>
        <id>https://dayuebai.github.io/post/about/</id>
        <link href="https://dayuebai.github.io/post/about/">
        </link>
        <updated>2019-01-23T17:09:48.000Z</updated>
    </entry>
</feed>